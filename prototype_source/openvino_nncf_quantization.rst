PyTorch 2 Export Quantization with NNCF quantization and OpenVINO runtime
===========================================================================

**Author**: dlyakhov, asuslov, aamir, # TODO: add required authors

Introduction
--------------

This tutorial introduces the steps for utilizing the `Neural Network Compression Framework (nncf) <https://github.com/openvinotoolkit/nncf/tree/develop>`_ to generate a quantized model customized
for the `OpenVINO torch.compile backend <https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html>`_ and explains how to lower the quantized model into the `OpenVINO <https://docs.openvino.ai/2024/index.html>`_ representation.

The pytorch 2 export quantization flow uses the torch.export to capture the model into a graph and perform quantization transformations on top of the ATen graph.
This approach is expected to have significantly higher model coverage, better programmability, and a simplified UX.
OpenVINO is the new backend that compiles the FX Graph generated by TorchDynamo into an optimized OpenVINO model.

The quantization flow mainly includes three steps:

- Step 1: OpenVINO and NNCF installation.
- Step 2: Capture the FX Graph from the eager Model based on the `torch export mechanism <https://pytorch.org/docs/main/export.html>`_.
- Step 3: Apply the Quantization flow based on the captured FX Graph.
- Step 4: Lower the quantized model into OpenVINO representation with the API ``torch.compile``.

The high-level architecture of this flow could look like this:

::

    float_model(Python)                          Example Input
        \                                              /
         \                                            /
    —--------------------------------------------------------
    |                         export                       |
    —--------------------------------------------------------
                                |
                        FX Graph in ATen
                                |
                                |
    —--------------------------------------------------------
    |                     nncf.quantize                     |
    —--------------------------------------------------------
                                |
                         Quantized Model
                                |
    —--------------------------------------------------------
    |                  Lower into Inductor                  |
    —--------------------------------------------------------
                                |
                          OpenVINO model

Post Training Quantization
----------------------------

Now, we will walk you through a step-by-step tutorial for how to use it with `torchvision resnet18 model <https://download.pytorch.org/models/resnet18-f37072fd.pth>`_
for post training quantization.

1. OpenVINO and NNCF installation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OpenVINO and NNCF could be easily installed via `pip distribution <https://docs.openvino.ai/2024/get-started/install-openvino.html>`_:

.. code-block:: bash

    pip install -U pip
    pip install openvino, nncf


2. Capture FX Graph
^^^^^^^^^^^^^^^^^^^^^

We will start by performing the necessary imports, capturing the FX Graph from the eager module.

.. code-block:: python

    import torch
    import torchvision.models as models
    import copy
    import openvino.torch

    import nncf
    from nncf.torch import disable_patching

    # Create the Eager Model
    model_name = "resnet18"
    model = models.__dict__[model_name](pretrained=True)

    # Set the model to eval mode
    model = model.eval()

    # Create the data, using the dummy data here as an example
    traced_bs = 50
    x = torch.randn(traced_bs, 3, 224, 224).contiguous(memory_format=torch.channels_last)
    example_inputs = (x,)

    # Capture the FX Graph to be quantized
    with torch.no_grad():
        with disable_patching():
            exported_model = torch.export.export(model, example_inputs).module()


Next, we will have the FX Module to be quantized.

3. Apply Quantization
^^^^^^^^^^^^^^^^^^^^^^^

Before the quantization, we need to create an instance of the nncf.Dataset class that represents the calibration dataset.
The ``nncf.Dataset`` class can be a wrapper over the framework dataset object that is used for model training or validation
The class constructor receives the dataset object and an optional transformation function.

The transformation function is a function that takes a sample from the dataset and returns data that can be passed to the model for inference.
For example, this function can take a tuple of a data tensor and labels tensor and return the former while ignoring the latter.
The transformation function is used to avoid modifying the dataset code to make it compatible with the quantization API.
The function is applied to each sample from the dataset before passing it to the model for inference.
The following code snippet shows how to create an instance of the ``nncf.Dataset`` class:

.. code-block:: python

    calibration_loader = torch.utils.data.DataLoader([example_inputs])

    def transform_fn(data_item):
        # In the transformation function,
        # user can separate labels and input data
        # from the given data item:
        # images, _ = data_item
        return data_item

    calibration_dataset = nncf.Dataset(calibration_loader, transform_fn)

If there is no framework dataset object, you can create your own entity that implements the Iterable interface in Python,
for example, the list of images, and returns data samples feasible for inference. In this case, a transformation function is not required.

Once the dataset is ready and the model object is instantiated, you can apply 8-bit quantization to it.

.. code-block:: python

    with disable_patching():
        quantized_model = nncf.quantize(exported_model, calibration_dataset)

``nncf.quantize()`` function has several optional parameters that allow tuning the quantization process to get a more accurate model.
Below is the list of parameters and their description:

* ``model_type`` - used to specify quantization scheme required for specific type of the model. Transformer is the only supported special quantization scheme to preserve accuracy after quantization of Transformer models (BERT, DistilBERT, etc.). None is default, i.e. no specific scheme is defined.
.. code-block:: python

    nncf.quantize(model, dataset, model_type=nncf.ModelType.Transformer)

* ``preset`` - defines quantization scheme for the model. Two types of presets are available:

    * ``PERFORMANCE`` (default) - defines symmetric quantization of weights and activations

    * ``MIXED`` - weights are quantized with symmetric quantization and the activations are quantized with asymmetric quantization. This preset is recommended for models with non-ReLU and asymmetric activation functions, e.g. ELU, PReLU, GELU, etc.

.. code-block:: python

    nncf.quantize(model, dataset, preset=nncf.QuantizationPreset.MIXED)

* ``fast_bias_correction`` - when set to False, enables a more accurate bias (error) correction algorithm that can be used to improve the accuracy of the model. True is used by default to minimize quantization time.

.. code-block:: python

    nncf.quantize(model, dataset, fast_bias_correction=False)

* ``subset_size`` - defines the number of samples from the calibration dataset that will be used to estimate quantization parameters of activations. The default value is 300.

.. code-block:: python

    nncf.quantize(model, dataset, subset_size=1000)

* ``ignored_scope`` - this parameter can be used to exclude some layers from the quantization process to preserve the model accuracy.  For example, when you want to exclude the last layer of the model from quantization.  Below are some examples of how to use this parameter:

.. code-block:: python

    #Exclude by layer name:
    names = ['layer_1', 'layer_2', 'layer_3']
    nncf.quantize(model, dataset, ignored_scope=nncf.IgnoredScope(names=names))

    #Exclude by layer type:
    types = ['Conv2d', 'Linear']
    nncf.quantize(model, dataset, ignored_scope=nncf.IgnoredScope(types=types))

    #Exclude by regular expression:
    regex = '.*layer_.*'
    nncf.quantize(model, dataset, ignored_scope=nncf.IgnoredScope(patterns=regex))

    #Exclude by subgraphs:
    # In this case, all nodes along all simple paths in the graph
    # from input to output nodes will be excluded from the quantization process.
    subgraph = nncf.Subgraph(inputs=['layer_1', 'layer_2'], outputs=['layer_3'])
    nncf.quantize(model, dataset, ignored_scope=nncf.IgnoredScope(subgraphs=[subgraph]))


* ``target_device`` - defines the target device, the specificity of which will be taken into account during optimization. The following values are supported: ``ANY`` (default), ``CPU``, ``CPU_SPR``, ``GPU``, and ``NPU``.

.. code-block:: python

    nncf.quantize(model, dataset, target_device=nncf.TargetDevice.CPU)

* ``advanced_parameters`` - used to specify advanced quantization parameters for fine-tuning the quantization algorithm.  Defined by nncf.quantization.advanced_parameters NNCF submodule.  None is default.

After these steps, we finished running the quantization flow, and we will get the quantized model.


4. Lower into OpenVINO representation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

After that the FX Graph can utilize OpenVINO optimizations using `torch.compile(…, backend=”openvino”) <https://docs.openvino.ai/2024/openvino-workflow/torch-compile.html>`_ functionality.

.. code-block:: python

    with torch.no_grad():
        optimized_model = torch.compile(quantized_model, backend="openvino")

        # Running some benchmark
        optimized_model(*example_inputs)


The optimized model is using low-level kernels designed specifically for Intel CPU.
This should significantly speed up inference time in comparison with the eager model.

Conclusion
------------

With this tutorial, we introduce how to use torch.compile with the OpenVINO backend with models quantized via ``nncf.quantize``.
For further information, please visit `complete example on renset18 model <https://github.com/openvinotoolkit/nncf/tree/v2.14.0/examples/post_training_quantization/torch_fx/resnet18>`_.
